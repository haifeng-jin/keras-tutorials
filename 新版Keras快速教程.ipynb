{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "新版Keras快速教程",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhGbgsE3NnEY"
      },
      "source": [
        "# 新版Keras快速教程\n",
        "\n",
        "---\n",
        "\n",
        "翻译自[Keras官网教程](https://keras.io/getting_started/intro_to_keras_for_researchers/)。\n",
        "\n",
        "译者：[金海峰](https://www.zhihu.com/people/jin-hai-feng-68)，Keras团队的一位中国小哥。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evb_UlRzNnEe"
      },
      "source": [
        "## 简介\n",
        "\n",
        "本教程将帮你掌握Keras和TensorFlow的基本用法和核心概念，其中包括：\n",
        "\n",
        "- 张量（tensor），变量（variable），梯度（gradient）。\n",
        "- 通过继承`Layer`类来建立自定义的神经网络层。\n",
        "- 手写循环来更好地自定义对神经网络训练过程。\n",
        "- 使用`add_loss()`方法来自定义神经网络层的损失函数。\n",
        "- 在手写循环训练过程中对评估标准（metrics）进行追踪。\n",
        "- 使用`tf.function`来编译并加速神经网络的执行过程。\n",
        "- 神经网络层的运行：训练模式 vs 推断（inference）模式。\n",
        "- Keras的函数式API。\n",
        "\n",
        "我们还会使用两个完整的例子来展示如何在实践中使用Keras。\n",
        "\n",
        "这两个例子分别是：\n",
        "变分自编码器（Variational Autoencoder）和Hypernetwork。\n",
        "\n",
        "首先，我们要import tensorflow和keras。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjTfl_5QNnEe"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYBZBiFUNnEe"
      },
      "source": [
        "## 张量（Tensors）\n",
        "\n",
        "TensorFlow是一个可微分编程的基础框架。\n",
        "它的核心功能就是对多维数组（张量）进行操作。\n",
        "这一点和NumPy类似。\n",
        "\n",
        "但是，它和NumPy还有很多重要的不同点，例举如下：\n",
        "\n",
        "- TensorFlow能使用硬件加速器（GPU和TPU）来加速运算。\n",
        "- TensorFlow能够对各式各样的张量运算表达式进行自动求导。\n",
        "- TensorFlow能够在多台机器或者一台机器的多个运算设备上进行分布式运算。\n",
        "\n",
        "我们首先来看一下TensorFlow的一个核心类，`Tensor`。\n",
        "\n",
        "如下代码建立了一个常量`Tensor`："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyn3vjwXNnEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "debbb434-c4a7-4c28-a10c-4f95e3f72dc0"
      },
      "source": [
        "x = tf.constant([[5, 2], [1, 3]])\n",
        "print(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[5 2]\n",
            " [1 3]], shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCT2Ob4wNnEf"
      },
      "source": [
        "你可以通过调用它的`.numpy()`方法来获取它的值。\n",
        "该方法会返回一个NumPy数组。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gGZZd49NnEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65060d95-2596-4ed2-c182-18b873a6dc47"
      },
      "source": [
        "x.numpy()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5, 2],\n",
              "       [1, 3]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPKW4dOkNnEg"
      },
      "source": [
        "与NumPy数组类似，一个`Tensor`对象也有`dtype`和`shape`这两个属性。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdFpam-oNnEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb64680d-a95f-4908-f992-8744c6e17912"
      },
      "source": [
        "print(\"dtype:\", x.dtype)\n",
        "print(\"shape:\", x.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dtype: <dtype: 'int32'>\n",
            "shape: (2, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic4hu4miNnEg"
      },
      "source": [
        "一种常用的简历常量`Tensor`的方法就是使用`tf.ones`和`tf.zeros`，和`np.ones`以及`np.zeros`的用法相同。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXwGA3TWNnEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f7b3ce-0e30-429a-dc22-40d712cd86a8"
      },
      "source": [
        "print(tf.ones(shape=(2, 1)))\n",
        "print(tf.zeros(shape=(2, 1)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1.]\n",
            " [1.]], shape=(2, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0.]\n",
            " [0.]], shape=(2, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwLWKMPqNnEh"
      },
      "source": [
        "你也可以生成随机的常量`Tensor`。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvRC3i0lNnEh"
      },
      "source": [
        "x = tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0)\n",
        "\n",
        "x = tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype=\"int32\")\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoFmTw--NnEh"
      },
      "source": [
        "## 变量（Variables）\n",
        "\n",
        "`Variable`类是一种特殊的张量，它可以用来承载一些可以改变其数值的张量，例如：神经网络的权重。\n",
        "\n",
        "你可以建立一个`Variable`对象，并指定初始值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8J2hqQMNnEi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b38418-5995-4da8-92fa-c78114c0c434"
      },
      "source": [
        "initial_value = tf.random.normal(shape=(2, 2))\n",
        "a = tf.Variable(initial_value)\n",
        "print(a)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
            "array([[ 0.72645265, -0.4716139 ],\n",
            "       [-1.9029021 , -0.5134245 ]], dtype=float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liAvj4_gNnEi"
      },
      "source": [
        "你可以用`.assign(...)`、`.assign_add(...)`和`.assign_sub(...)`来为标量指定新的值、加上一个值和减去一个值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1RbqipxNnEi"
      },
      "source": [
        "new_value = tf.random.normal(shape=(2, 2))\n",
        "a.assign(new_value)\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        assert a[i, j] == new_value[i, j]\n",
        "\n",
        "added_value = tf.random.normal(shape=(2, 2))\n",
        "a.assign_add(added_value)\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        assert a[i, j] == new_value[i, j] + added_value[i, j]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNJWSrNWNnEi"
      },
      "source": [
        "## 使用TensorFlow来进行数学运算\n",
        "\n",
        "用TensorFlow进行数学运算和NumPy非常类似。\n",
        "最主要的区别是TensorFlow能在GPU和TPU上面运行。\n",
        "\n",
        "这是一些基本的矩阵运算。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfbKFYjFNnEi"
      },
      "source": [
        "a = tf.random.normal(shape=(2, 2))\n",
        "b = tf.random.normal(shape=(2, 2))\n",
        "\n",
        "c = a + b\n",
        "d = tf.square(c)\n",
        "e = tf.exp(d)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb690v-9NnEj"
      },
      "source": [
        "## 梯度（Gradients）\n",
        "\n",
        "另一个TensorFlow和NumPy的主要区别就是可以自动对可微的表达式求导并计算梯度。\n",
        "\n",
        "你可是用`GradientTape`的`tape.watch()`方法来监视一个张量的相关运算，并把要求导的表达式和该张量作为参数传入`tape.gradient(...)`方法，就可以得到梯度了。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4ZVOJZUNnEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de86597-14ad-425f-edf3-cf56f3438b57"
      },
      "source": [
        "a = tf.random.normal(shape=(2, 2))\n",
        "b = tf.random.normal(shape=(2, 2))\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(a)  # 对`a`的所有运算进行记录\n",
        "    c = tf.sqrt(tf.square(a) + tf.square(b))  # 对`a`进行一些数学运算\n",
        "    # `c`对`a`的梯度是多少？\n",
        "    dc_da = tape.gradient(c, a)\n",
        "    print(dc_da)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.49744755  0.2795373 ]\n",
            " [ 0.90746343  0.94444644]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1fFz76QNnEk"
      },
      "source": [
        "在默认情况下，所有变量都会被自动监视，你不需要特意调用`watch()`。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io5daNEVNnEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22c08b3-9b00-49b4-bca4-068b17596074"
      },
      "source": [
        "a = tf.Variable(a)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    c = tf.sqrt(tf.square(a) + tf.square(b))\n",
        "    dc_da = tape.gradient(c, a)\n",
        "    print(dc_da)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.49744755  0.2795373 ]\n",
            " [ 0.90746343  0.94444644]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_q4N6cbNnEl"
      },
      "source": [
        "如果你把`GradientTape`嵌套起来，就可以求高阶导数啦。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-pFjGALNnEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b19278-887d-4dbf-e3e2-9ccb8941e66f"
      },
      "source": [
        "with tf.GradientTape() as outer_tape:\n",
        "    with tf.GradientTape() as tape:\n",
        "        c = tf.sqrt(tf.square(a) + tf.square(b))\n",
        "        dc_da = tape.gradient(c, a)\n",
        "    d2c_da2 = outer_tape.gradient(dc_da, a)\n",
        "    print(d2c_da2)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.54047406 1.3902092 ]\n",
            " [0.18874693 0.06999791]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkDi_4rVNnEm"
      },
      "source": [
        "## Keras中的神经网络层（Layer）\n",
        "\n",
        "TensorFlow是**可微分编程的基础框架**，主要用于处理张量、变量和梯度。\n",
        "Keras是**深度学习的用户接口**，主要用于处理神经网络层、模型、优化器、损失函数、评估标准等等。\n",
        "\n",
        "Keras作为TensorFlow的上层API，使得TensorFlow更加简单用，以提高工程师们的生产力。\n",
        "\n",
        "`Layer`类是Keras中最基础的一种抽象，表示神经网络的层。\n",
        "它对权重和一些（在`call()`方法中定义的）计算表达式进行了封装。\n",
        "\n",
        "下面是一个简单的`Layer`的例子。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNAm2wZvNnEm"
      },
      "source": [
        "\n",
        "class Linear(keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super(Linear, self).__init__()\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )\n",
        "        b_init = tf.zeros_initializer()\n",
        "        self.b = tf.Variable(\n",
        "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuxsALU0NnEm"
      },
      "source": [
        "你可以把`Layer`的对象当成一个Python函数来用。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7x1iz0INnEm"
      },
      "source": [
        "# 初始化这个Layer。\n",
        "linear_layer = Linear(units=4, input_dim=2)\n",
        "\n",
        "# 我们可以像调用函数一样调用它，传给它一些数据。\n",
        "y = linear_layer(tf.ones((2, 2)))\n",
        "assert y.shape == (2, 4)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU1Ty2AdNnEn"
      },
      "source": [
        "在`__init__`函数中简历的权重变量会自动被追踪，它们都被放在了`weights`这个属性里面。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNY02UWzNnEn"
      },
      "source": [
        "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtY1EtubNnEn"
      },
      "source": [
        "Keras里面有很多内置的Layer可以使用，比如：`Dense`、`Conv2D`、`LSTM`。\n",
        "\n",
        "还有一些更神奇的Layer，比如：`Conv3DTranspose`和`ConvLSTM2D`。\n",
        "\n",
        "这些内置的Layer能帮你在编写神经网络的时候省点事儿。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXi4anO3NnEn"
      },
      "source": [
        "## 给Layer添加权重（weights）\n",
        "\n",
        "一种比较方便的方法就是用`self.add_weight()`来添加权重。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdjK0mqQNnEn"
      },
      "source": [
        "\n",
        "class Linear(keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "    def __init__(self, units=32):\n",
        "        super(Linear, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "\n",
        "# 初始化Layer。它会延迟加载权重。\n",
        "linear_layer = Linear(4)\n",
        "\n",
        "# 下面这句才会调用`build(input_shape)`，并添加权重。\n",
        "y = linear_layer(tf.ones((2, 2)))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jMZBQDkNnEo"
      },
      "source": [
        "## Layer的梯度\n",
        "\n",
        "你可以通过在`GradientTape`语句里面来调用一个Layer来自动获取它的梯度。\n",
        "通过梯度，你可以更新一个Layer的权重。\n",
        "你可以使用优化器来进行自动更新，也可以自己写代码来更新。\n",
        "当然，如果有必要的话，你也可以在更新权重之前对梯度进行修改。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ40kWAtNnEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39cf091d-b44b-4784-ec62-8918d0435e89"
      },
      "source": [
        "# 数据准备\n",
        "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
        ")\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# 初始化这个线性Layer（在上面的代码中定义的），让它有10个神经元。\n",
        "linear_layer = Linear(10)\n",
        "\n",
        "# 初始化一个逻辑损失函数，接受整数值的标签作为输入。\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# 初始化一个优化器。\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "# 循环数据集中所有的batch\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "\n",
        "    # 创建一个GradientTape\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # 正向传播\n",
        "        logits = linear_layer(x)\n",
        "\n",
        "        # 获取对于当前batch的损失函数值\n",
        "        loss = loss_fn(y, logits)\n",
        "\n",
        "    # 求损失函数关于权重的梯度\n",
        "    gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
        "\n",
        "    # 更新线性Layer的权重\n",
        "    optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\n",
        "\n",
        "    # 输出日志\n",
        "    if step % 100 == 0:\n",
        "        print(\"Step:\", step, \"Loss:\", float(loss))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Step: 0 Loss: 2.425366163253784\n",
            "Step: 100 Loss: 2.2906112670898438\n",
            "Step: 200 Loss: 2.119081497192383\n",
            "Step: 300 Loss: 2.087733268737793\n",
            "Step: 400 Loss: 2.002962112426758\n",
            "Step: 500 Loss: 1.927724838256836\n",
            "Step: 600 Loss: 1.8112218379974365\n",
            "Step: 700 Loss: 1.7492737770080566\n",
            "Step: 800 Loss: 1.715962529182434\n",
            "Step: 900 Loss: 1.5601446628570557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVlhso5LNnEo"
      },
      "source": [
        "## 可训练（trainable）和不可训练（non-trainable）的权重\n",
        "\n",
        "Layer中的权重可以是可以训练的，也可以是不可训练的。\n",
        "他们分别存在了`trainalbe_weights`和`non_trainable_weights`属性中。\n",
        "下面是一个含有不可训练权重的Layer的例子。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVoKoqaeNnEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26671775-e4a1-4d1d-8cdb-b9203ec3a85e"
      },
      "source": [
        "\n",
        "class ComputeSum(keras.layers.Layer):\n",
        "    \"\"\"Returns the sum of the inputs.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        super(ComputeSum, self).__init__()\n",
        "        # 建立一个不可训练的权重\n",
        "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
        "        return self.total\n",
        "\n",
        "\n",
        "my_sum = ComputeSum(2)\n",
        "x = tf.ones((2, 2))\n",
        "\n",
        "y = my_sum(x)\n",
        "print(y.numpy())  # [2. 2.]\n",
        "\n",
        "y = my_sum(x)\n",
        "print(y.numpy())  # [4. 4.]\n",
        "\n",
        "assert my_sum.weights == [my_sum.total]\n",
        "assert my_sum.non_trainable_weights == [my_sum.total]\n",
        "assert my_sum.trainable_weights == []"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2. 2.]\n",
            "[4. 4.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KDRkVNoNnEp"
      },
      "source": [
        "## Layer的嵌套使用\n",
        "\n",
        "Layer可以被递归式地嵌套使用，\n",
        "这样可以更好地把更多的计算过程包进一个Layer里面。\n",
        "每个Layer都会自动追踪它所包含的Layer的权重，无论是可训练的还是不可训练的都是。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCl9KoJNNnEp"
      },
      "source": [
        "# 我们建立一个多层感知机（MLP）Layer\n",
        "# 它包含了我们之前实现的线性Layer\n",
        "\n",
        "\n",
        "class MLP(keras.layers.Layer):\n",
        "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.linear_1 = Linear(32)\n",
        "        self.linear_2 = Linear(32)\n",
        "        self.linear_3 = Linear(10)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.linear_1(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return self.linear_3(x)\n",
        "\n",
        "\n",
        "mlp = MLP()\n",
        "\n",
        "# 下面这个句首次调用了`mlp`，也就添加了所有权重\n",
        "y = mlp(tf.ones(shape=(3, 64)))\n",
        "\n",
        "# 权重是被递归式地追踪的\n",
        "assert len(mlp.weights) == 6"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w34pdHAZNnEq"
      },
      "source": [
        "上面我们手动实现的多层感知机（MLP）与如下使用内置层实现的代码是等价的。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8rs7H8ANnEq"
      },
      "source": [
        "mlp = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
        "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
        "        keras.layers.Dense(10),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqKC8mDGNnEq"
      },
      "source": [
        "## 追踪Layer所产生的损失函数\n",
        "\n",
        "Layer可是在正向传播过程中使用`add_loss()`方法来添加损失函数。\n",
        "一种很好的把正则化加入损失函数的方法。\n",
        "子Layer所添加的损失函数也会自动被母Layer所追踪。\n",
        "\n",
        "下面的例子是把activity正则化加入了损失函数。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXWoRVAUNnEq"
      },
      "source": [
        "\n",
        "class ActivityRegularization(keras.layers.Layer):\n",
        "    \"\"\"一个把activity稀疏正则化加入损失函数的Layer。\"\"\"\n",
        "\n",
        "    def __init__(self, rate=1e-2):\n",
        "        super(ActivityRegularization, self).__init__()\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # 使用`add_loss`来添加正则化损失函数，用输入的张量计算\n",
        "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
        "        return inputs\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dERNrMoKNnEr"
      },
      "source": [
        "一个模型只要使用了这个Layer，就会自动把这个正则化的损失函数加入到整体损失函数中。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWRMm39pNnEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81764f55-1196-4ba5-ce17-5ba22449d12e"
      },
      "source": [
        "# 在多层感知机中使用该损失函数。\n",
        "\n",
        "\n",
        "class SparseMLP(keras.layers.Layer):\n",
        "    \"\"\"一系列线性Layer，包含稀疏正则化损失函数。\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SparseMLP, self).__init__()\n",
        "        self.linear_1 = Linear(32)\n",
        "        self.regularization = ActivityRegularization(1e-2)\n",
        "        self.linear_3 = Linear(10)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.linear_1(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.regularization(x)\n",
        "        return self.linear_3(x)\n",
        "\n",
        "\n",
        "mlp = SparseMLP()\n",
        "y = mlp(tf.ones((10, 10)))\n",
        "\n",
        "print(mlp.losses)  # 一个包含一个float32类型标量的列表"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(), dtype=float32, numpy=0.21000613>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhJ_JIAQNnEr"
      },
      "source": [
        "在每次正向传播开始，最外层的Layer会先把损失函数值清空，以防止把上一次正向传播产生的损失函数加进来。`layer.loss`永远只包含最近一次完成的正向传播所产生的的损失函数。你在自己写训练循环的时候，在用这些损失函数来计算梯度之前，你需要先把他们加起来求和。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8Fsisp_NnEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42faff5-518f-4a0d-d693-af9965029d4f"
      },
      "source": [
        "# 最近一次正向传播所产生的损失函数\n",
        "mlp = SparseMLP()\n",
        "mlp(tf.ones((10, 10)))\n",
        "assert len(mlp.losses) == 1\n",
        "mlp(tf.ones((10, 10)))\n",
        "assert len(mlp.losses) == 1  # 不会包含之前的损失函数\n",
        "\n",
        "# 我们来演示一下怎样在训练循环中使用这些损失函数\n",
        "\n",
        "# 准备数据\n",
        "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
        ")\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# 一个新的多层感知机对象\n",
        "mlp = SparseMLP()\n",
        "\n",
        "# 损失函数和优化器\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # 正向传播.\n",
        "        logits = mlp(x)\n",
        "\n",
        "        # 对于当前batch的默认的损失函数值\n",
        "        loss = loss_fn(y, logits)\n",
        "\n",
        "        # 加上在正向传播过程中所产生的损失函数值\n",
        "        loss += sum(mlp.losses)\n",
        "\n",
        "        # 求损失函数关于权重的梯度\n",
        "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
        "\n",
        "    # 对线性Layer的权重进行更新\n",
        "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
        "\n",
        "    # 输出日志\n",
        "    if step % 100 == 0:\n",
        "        print(\"Step:\", step, \"Loss:\", float(loss))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0 Loss: 6.507600784301758\n",
            "Step: 100 Loss: 2.5422661304473877\n",
            "Step: 200 Loss: 2.4140090942382812\n",
            "Step: 300 Loss: 2.3433475494384766\n",
            "Step: 400 Loss: 2.345116376876831\n",
            "Step: 500 Loss: 2.342367172241211\n",
            "Step: 600 Loss: 2.330716848373413\n",
            "Step: 700 Loss: 2.3208255767822266\n",
            "Step: 800 Loss: 2.3258860111236572\n",
            "Step: 900 Loss: 2.3363797664642334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2-t-KgbNnEr"
      },
      "source": [
        "## 追踪训练的评估标准（Metrics）\n",
        "\n",
        "Keras有很多的内置评估标准，比如`tf.keras.metrics.AUC`和`tf.keras.metrics.PrecisionAtRecall`。\n",
        "你也可以写一个你自己的评估标准，几行代码就能搞定。\n",
        "\n",
        "在一个手写的训练循环里面使用评估标准，你要做如下几件事：\n",
        "- 初始化评估标准，比如`tf.keras.metrics.AUC()`。\n",
        "- 在每个batch中调用函数`metric.update_state(targets, predictions)`来更新评估标准的统计值。\n",
        "- 用`metric.results()`来查询评估标准当前的值。\n",
        "- 在每个epoch结尾或者开始时用`metric.reset_states()`来重置评估标准的值。\n",
        "\n",
        "下面是一个简单的例子。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5aEwv7NNnEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f6a809-9228-4603-e538-08dc9292be71"
      },
      "source": [
        "# 初始化评估标准对象\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "# 准备好了我们的Layer、损失函数和优化器\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.Dense(32, activation=\"relu\"),\n",
        "        keras.layers.Dense(32, activation=\"relu\"),\n",
        "        keras.layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "for epoch in range(2):\n",
        "    # 循环数据集里所有的batch\n",
        "    for step, (x, y) in enumerate(dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x)\n",
        "            # 计算当前batch的损失函数值\n",
        "            loss_value = loss_fn(y, logits)\n",
        "\n",
        "        # 更新`accuracy`（准确率）评估标准的统计值\n",
        "        accuracy.update_state(y, logits)\n",
        "\n",
        "        # 更新模型权重，以最小化损失函数值\n",
        "        gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "        # 输出当前的评估标准值到日志\n",
        "        if step % 200 == 0:\n",
        "            print(\"Epoch:\", epoch, \"Step:\", step)\n",
        "            print(\"Total running accuracy so far: %.3f\" % accuracy.result())\n",
        "\n",
        "    # 在epoch结尾重置评估标准的值\n",
        "    accuracy.reset_states()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Step: 0\n",
            "Total running accuracy so far: 0.125\n",
            "Epoch: 0 Step: 200\n",
            "Total running accuracy so far: 0.771\n",
            "Epoch: 0 Step: 400\n",
            "Total running accuracy so far: 0.836\n",
            "Epoch: 0 Step: 600\n",
            "Total running accuracy so far: 0.862\n",
            "Epoch: 0 Step: 800\n",
            "Total running accuracy so far: 0.877\n",
            "Epoch: 1 Step: 0\n",
            "Total running accuracy so far: 0.938\n",
            "Epoch: 1 Step: 200\n",
            "Total running accuracy so far: 0.938\n",
            "Epoch: 1 Step: 400\n",
            "Total running accuracy so far: 0.943\n",
            "Epoch: 1 Step: 600\n",
            "Total running accuracy so far: 0.944\n",
            "Epoch: 1 Step: 800\n",
            "Total running accuracy so far: 0.944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgG92NuINnEs"
      },
      "source": [
        "和`self.add_loss()`类似，你也可以使用`self.add_metric()`来添加任何一个变量进去作为评估标准。可以用`layer.reset_metrics()`来重置某一层或整个模型的评估标准的值。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYXMlRyDNnEs"
      },
      "source": [
        "## 函数编译\n",
        "\n",
        "默认执行时我们使用的是Eager模式，调试程序比较方便。\n",
        "与之对应的静态图模式则在执行时更快。\n",
        "你只要把一个函数用`@tf.function`这个装饰器装饰一下，那个函数就会以静态图模式执行了。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP7ST4pxNnEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de7fe328-00b6-4059-801b-8502d10f7142"
      },
      "source": [
        "# 准备好模型、损失函数和优化器\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.Dense(32, activation=\"relu\"),\n",
        "        keras.layers.Dense(32, activation=\"relu\"),\n",
        "        keras.layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "# 建立一个训练函数，每次执行一个batch\n",
        "\n",
        "\n",
        "@tf.function  # 用这句来加速\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x)\n",
        "        loss = loss_fn(y, logits)\n",
        "        gradients = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "    return loss\n",
        "\n",
        "\n",
        "# 准备数据\n",
        "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
        ")\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "    loss = train_on_batch(x, y)\n",
        "    if step % 100 == 0:\n",
        "        print(\"Step:\", step, \"Loss:\", float(loss))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0 Loss: 2.3842484951019287\n",
            "Step: 100 Loss: 0.4142776131629944\n",
            "Step: 200 Loss: 0.2205623984336853\n",
            "Step: 300 Loss: 0.5111865997314453\n",
            "Step: 400 Loss: 0.3792179822921753\n",
            "Step: 500 Loss: 0.22862757742404938\n",
            "Step: 600 Loss: 0.14353641867637634\n",
            "Step: 700 Loss: 0.25944337248802185\n",
            "Step: 800 Loss: 0.17433446645736694\n",
            "Step: 900 Loss: 0.10836410522460938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2LCrk-yNnEt"
      },
      "source": [
        "## 训练模式和推断（inference）模式\n",
        "\n",
        "某些神经网络的层在训练和推断的时候采用的是不同的计算公式，也就是说，他们在训练和推断的时候做的事情是不一样的。\n",
        "对于这种情况，我们有一种标准操作来帮你拿到现在究竟是在训练还是在推断这个信息。\n",
        "那就是在`call`函数里面有一个布尔型参数叫`training`。\n",
        "\n",
        "在你自定义层的`call`函数里面使用这个参数，可以帮你正确地实现好该层在训练和推断时候的不同表现。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yGHXzM2NnEt"
      },
      "source": [
        "\n",
        "class Dropout(keras.layers.Layer):\n",
        "    def __init__(self, rate):\n",
        "        super(Dropout, self).__init__()\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            return tf.nn.dropout(inputs, rate=self.rate)\n",
        "        return inputs\n",
        "\n",
        "\n",
        "class MLPWithDropout(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(MLPWithDropout, self).__init__()\n",
        "        self.linear_1 = Linear(32)\n",
        "        self.dropout = Dropout(0.5)\n",
        "        self.linear_3 = Linear(10)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        x = self.linear_1(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return self.linear_3(x)\n",
        "\n",
        "\n",
        "mlp = MLPWithDropout()\n",
        "y_train = mlp(tf.ones((2, 2)), training=True)\n",
        "y_test = mlp(tf.ones((2, 2)), training=False)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTS4MJF_NnEt"
      },
      "source": [
        "## 用函数式API来建立模型\n",
        "\n",
        "要建立模型的话，你不一定非要用面向对象的方法。我们之前所看到的Layer是可以用函数式的方法来组合到一起的，我们称之为函数式API。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVEZn6mlNnEt"
      },
      "source": [
        "# 用`Input`对象来指定输入的尺寸和类型，就像我们给变量定义一个类型一样\n",
        "# `shape`是用来描述单个样本的尺寸的，不包含batch大小这个维度\n",
        "# 函数式API旨在定义单个样本在模型里面的变化过程\n",
        "# 模型会自动把这些对单个样本的操作打包成对batch的操作，然后按batch来处理数据\n",
        "inputs = tf.keras.Input(shape=(16,), dtype=\"float32\")\n",
        "\n",
        "# 我们把这些Layer当函数调用，并输入这些标记着变量类型和尺寸的对象\n",
        "# 我们会收到一个标记着新的尺寸和类型的对象\n",
        "x = Linear(32)(inputs)  # 使用我们之前定义的线性Layer\n",
        "x = Dropout(0.5)(x)  # 使用我们之前定义的Dropout层\n",
        "outputs = Linear(10)(x)\n",
        "\n",
        "# 一个函数式的`Model`对象可以用它的输入和输出来进行定义\n",
        "# 一个`Model`对象本身也是一个`Layer`\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# 一个函数式的模型在我们用数据调用它之前就已经有了权重\n",
        "# 因为我们建立模型时就定义了它的输入尺寸，它也就可以推理出所有权重的尺寸了\n",
        "assert len(model.weights) == 4\n",
        "\n",
        "# 我们来试着把一些数据输入到模型里面\n",
        "y = model(tf.ones((2, 16)))\n",
        "assert y.shape == (2, 10)\n",
        "\n",
        "# 我们也可以给`__call__`函数发一个`training`的参数\n",
        "# 这个参数会一直深入传递到Dropout层\n",
        "y = model(tf.ones((2, 16)), training=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEBiQZtYNnEt"
      },
      "source": [
        "函数式API比继承的方式更简洁，并且还有一些别的优势，即函数式编程相比于面向对象编程的一些优势。\n",
        "但是，函数式API只能用来定义一些有向无环图（DAG），对于循环神经网络来讲，我们就必须使用继承的方式来实现。\n",
        "\n",
        "[这里](https://keras.io/guides/functional_api/)有更详尽的函数式API教程。\n",
        "\n",
        "在你建立模型的过程中，可能通常要使用函数式和继承这两种建模方法的结合。\n",
        "\n",
        "`Model`类的另一个比较好的特性是有`fit()`和`evaluate()`这些内置的函数。你可以继承`Model`类，就像我们继承`Layer`类一样，然后重载这些函数来实现你自己的训练和评估循环。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw1JOT3GNnEu"
      },
      "source": [
        "## 完整例子1：变分自编码器（Variational Autoencoder, VAE）\n",
        "\n",
        "我们先来总结一下我们已经学到的内容：\n",
        "\n",
        "- 一个`Layer`对象可以封装一些状态值（在`__init__`或者`build`里面定义的权重等等）和一些计算式（在`call`里面定义的）。\n",
        "- Layer可以递归式地组合在一起来构建更大的包含更多计算的Layer。\n",
        "- 你可以自定义的训练循环，你只需要开启`GradientTape`，在其内部调用你的模型来获取梯度，并传递给优化器即可。\n",
        "- 你可以用`@tf.function`装饰器来给你的训练循环加速。\n",
        "- 可以用`self.add_loss()`来给Layer添加损失函数，一般是用来添加正则化的。\n",
        "\n",
        "我们接下来就把这些内容一起放进一个完整的例子里。我们来写一变分自编码器（Variational Autoencoder, VAE），并且使用MNIST数据集来进行训练。\n",
        "\n",
        "VAE会继承Layer类，并嵌套调用其他Layer。还会使用KL散度作为正则化的损失函数。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_7si5RENnEu"
      },
      "source": [
        "下面的代码就是用来建立模型的。\n",
        "\n",
        "首先，我们需要一个`Encoder`编码器类，它把一个MNIST手写数字图片来对应到一个在潜在空间（latent space）里面的三元组`(z_mean, z_log_var, z)`，过程中使用了一个`Sampling`采样层。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccFBRsGTNnEu"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"使用`(z_mean, z_log_var)`来采样获得对一个数字的编码z\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "class Encoder(layers.Layer):\n",
        "    \"\"\"把一个数字对应到一个三元组`(z_mean, z_log_var, z)`\"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim=32, intermediate_dim=64, **kwargs):\n",
        "        super(Encoder, self).__init__(**kwargs)\n",
        "        self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)\n",
        "        self.dense_mean = layers.Dense(latent_dim)\n",
        "        self.dense_log_var = layers.Dense(latent_dim)\n",
        "        self.sampling = Sampling()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense_proj(inputs)\n",
        "        z_mean = self.dense_mean(x)\n",
        "        z_log_var = self.dense_log_var(x)\n",
        "        z = self.sampling((z_mean, z_log_var))\n",
        "        return z_mean, z_log_var, z\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uVY5mwLNnEu"
      },
      "source": [
        "下一步，我们用一个`Decoder`解码器类来把潜在空间的坐标对应回到一个MNIST数字图片。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b7s8-rlNnEv"
      },
      "source": [
        "\n",
        "class Decoder(layers.Layer):\n",
        "    \"\"\"把编码成的向量z转化回原来的数字图片\"\"\"\n",
        "\n",
        "    def __init__(self, original_dim, intermediate_dim=64, **kwargs):\n",
        "        super(Decoder, self).__init__(**kwargs)\n",
        "        self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)\n",
        "        self.dense_output = layers.Dense(original_dim, activation=tf.nn.sigmoid)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense_proj(inputs)\n",
        "        return self.dense_output(x)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk-FD40PNnEw"
      },
      "source": [
        "最后，我们的`VariationalAutoEncoder`变分自编码器类会把编码器和解码器串起来，然后用`add_loss()`来加入KL散度正则化的损失函数。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbvvBo_YNnEw"
      },
      "source": [
        "\n",
        "class VariationalAutoEncoder(layers.Layer):\n",
        "    \"\"\"把编码器和解码器串起来形成一个完成的模型用于训练\"\"\"\n",
        "\n",
        "    def __init__(self, original_dim, intermediate_dim=64, latent_dim=32, **kwargs):\n",
        "        super(VariationalAutoEncoder, self).__init__(**kwargs)\n",
        "        self.original_dim = original_dim\n",
        "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
        "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstructed = self.decoder(z)\n",
        "        # 把KL散度正则化加入损失函数\n",
        "        kl_loss = -0.5 * tf.reduce_mean(\n",
        "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
        "        )\n",
        "        self.add_loss(kl_loss)\n",
        "        return reconstructed\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHcCnkgKNnEx"
      },
      "source": [
        "现在我们来写一个训练循环，我们的训练过程会使用`@tf.function`来编译成静态图来加速。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhYGtrG6NnEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79dd725e-3788-4ad7-e7e0-d5b614d6cd0e"
      },
      "source": [
        "# 初始化模型\n",
        "vae = VariationalAutoEncoder(original_dim=784, intermediate_dim=64, latent_dim=32)\n",
        "\n",
        "# 损失函数和优化器\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "# 准备数据\n",
        "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
        ")\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def training_step(x):\n",
        "    with tf.GradientTape() as tape:\n",
        "        reconstructed = vae(x)  # 计算出重建的输入图片\n",
        "        # 计算损失函数值\n",
        "        loss = loss_fn(x, reconstructed)\n",
        "        loss += sum(vae.losses)  # 加上KL散度的损失函数\n",
        "    # 更新VAE的权重\n",
        "    grads = tape.gradient(loss, vae.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "    return loss\n",
        "\n",
        "\n",
        "losses = []  # 用于记录过程中产生的损失函数值\n",
        "for step, x in enumerate(dataset):\n",
        "    loss = training_step(x)\n",
        "    # 输出日志\n",
        "    losses.append(float(loss))\n",
        "    if step % 100 == 0:\n",
        "        print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))\n",
        "\n",
        "    # 1000步之后停止\n",
        "    # 把模型训练至收敛就当成留给你的练习题了\n",
        "    if step >= 1000:\n",
        "        break"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0 Loss: 0.31538477540016174\n",
            "Step: 100 Loss: 0.12489071001510808\n",
            "Step: 200 Loss: 0.09933947263962001\n",
            "Step: 300 Loss: 0.0893392476726608\n",
            "Step: 400 Loss: 0.08442602329829388\n",
            "Step: 500 Loss: 0.08138602710293677\n",
            "Step: 600 Loss: 0.07897867765680526\n",
            "Step: 700 Loss: 0.0776745897960901\n",
            "Step: 800 Loss: 0.07644307679879234\n",
            "Step: 900 Loss: 0.07552914691470836\n",
            "Step: 1000 Loss: 0.07460356648866233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgRWOPteNnEx"
      },
      "source": [
        "由此可见，用Keras来建立和训练这样的模型是很快很容易实现的。\n",
        "\n",
        "现在你可能觉得上面的代码还是有点啰嗦，这是因为我们每个细节都是亲自用代码实现的。这让我们有了最大的自由度，但同时也增加了我们的工作量。\n",
        "\n",
        "我们看看用函数式API来实现的话怎么样。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWD1Ap1ANnEx"
      },
      "source": [
        "original_dim = 784\n",
        "intermediate_dim = 64\n",
        "latent_dim = 32\n",
        "\n",
        "# 编码器\n",
        "original_inputs = tf.keras.Input(shape=(original_dim,), name=\"encoder_input\")\n",
        "x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()((z_mean, z_log_var))\n",
        "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")\n",
        "\n",
        "# 解码器\n",
        "latent_inputs = tf.keras.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
        "x = layers.Dense(intermediate_dim, activation=\"relu\")(latent_inputs)\n",
        "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
        "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
        "\n",
        "# VAE模型\n",
        "outputs = decoder(z)\n",
        "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
        "\n",
        "# 添加KL散度正则化损失函数\n",
        "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "vae.add_loss(kl_loss)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T2qoRyJNnEy"
      },
      "source": [
        "Much more concise, right?\n",
        "\n",
        "By the way, Keras also features built-in training & evaluation loops on its `Model` class\n",
        "(`fit()` and `evaluate()`). Check it out:\n",
        "\n",
        "这样一来就简洁多了。\n",
        "\n",
        "另外，Keras还有内置的训练和评估循环，是`Model`类的方法（`fit()`和`evaluate()`）。代码如下："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcuW7I2SNnEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0871924f-3ebf-495c-8980-48b264735ec1"
      },
      "source": [
        "# 损失函数和优化器\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "# 准备数据\n",
        "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
        ")\n",
        "dataset = dataset.map(lambda x: (x, x))  # 用x_train同时作为输入和输出目标\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
        "\n",
        "# 配置模型，准备训练\n",
        "vae.compile(optimizer, loss=loss_fn)\n",
        "\n",
        "# 对模型进行训练\n",
        "vae.fit(dataset, epochs=1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc9ce1bdf10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhHa7NHBNnEy"
      },
      "source": [
        "使用函数式API的`fit`方法把我们的代码从之前的65行缩短到了25行（包括了模型的定义和训练）。\n",
        "Keras的设计原则是最大化你的生产力，同时也可以让你像我们两段之前的那个例子一样，自己实现和控制每个细节。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpfAwLmaNnEy"
      },
      "source": [
        "## 完整例子2：超网络（Hypernetworks）\n",
        "\n",
        "我们来看一下另一个实验：超网络（Hypernetworks）。\n",
        "\n",
        "超网络是一种特殊的深度神经网络，它的权重是用另一个神经网络生成出来的。（通常另一个神经网络要更小一些）。\n",
        "\n",
        "我们来实现一个非常简单的超网络。我们用一个两层的神经网络来输出另一个三层的神经网络的权重。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S98qm49ONnEy"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "input_dim = 784\n",
        "classes = 10\n",
        "\n",
        "# 这个是我们用来预测数据标签的模型\n",
        "outer_model = keras.Sequential(\n",
        "    [keras.layers.Dense(64, activation=tf.nn.relu), keras.layers.Dense(classes),]\n",
        ")\n",
        "\n",
        "# 它不需要建立自己的权重（而是另一个网络输出的），所以我们把它设置成已经完成了build过程\n",
        "# 这样我们调用上面的outer_model的时候，就不会自动产生新的权重了\n",
        "for layer in outer_model.layers:\n",
        "    layer.built = True\n",
        "\n",
        "# 算是下有多少个权重值需要另一个网络输出\n",
        "# 每个outer_model的层都需要output_dim * input_dim + output_dim个权重值。\n",
        "num_weights_to_generate = (classes * 64 + classes) + (64 * input_dim + 64)\n",
        "\n",
        "# 下面这个模型是用来输出outer_model的权重的。\n",
        "inner_model = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.Dense(16, activation=tf.nn.relu),\n",
        "        keras.layers.Dense(num_weights_to_generate, activation=tf.nn.sigmoid),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMGnQ0rdNnEy"
      },
      "source": [
        "在训练循环里，对于每个batch，我们要做如下几件事：\n",
        "\n",
        "- 用`inner_model`来生成权重数组`weights_pred`。\n",
        "- 修改权重数组的尺寸，变成和`outer_model`所需要的权重尺寸一样。\n",
        "- `outer_model`运行一次正向传播来输出对MNIST图片的预测结果。\n",
        "- 运行反向传播来更新`inner_model`自身的权重，来让分类损失函数最小化。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD3t6maHNnEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90db90bc-0c61-4e95-fb5f-cb82d7443ff7"
      },
      "source": [
        "# 损失函数和优化器\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "# 准备数据\n",
        "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
        ")\n",
        "\n",
        "# 我们把batch大小设置为1\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(1)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # 输出outer_model的权重\n",
        "        weights_pred = inner_model(x)\n",
        "\n",
        "        # 改变输出权重的尺寸，编程和w、b一样的尺寸给outer_model.\n",
        "        # 第0层的kernel矩阵\n",
        "        start_index = 0\n",
        "        w0_shape = (input_dim, 64)\n",
        "        w0_coeffs = weights_pred[:, start_index : start_index + np.prod(w0_shape)]\n",
        "        w0 = tf.reshape(w0_coeffs, w0_shape)\n",
        "        start_index += np.prod(w0_shape)\n",
        "        # 第0层的偏移向量\n",
        "        b0_shape = (64,)\n",
        "        b0_coeffs = weights_pred[:, start_index : start_index + np.prod(b0_shape)]\n",
        "        b0 = tf.reshape(b0_coeffs, b0_shape)\n",
        "        start_index += np.prod(b0_shape)\n",
        "        # 第1层的kernel矩阵\n",
        "        w1_shape = (64, classes)\n",
        "        w1_coeffs = weights_pred[:, start_index : start_index + np.prod(w1_shape)]\n",
        "        w1 = tf.reshape(w1_coeffs, w1_shape)\n",
        "        start_index += np.prod(w1_shape)\n",
        "        # 第1层的偏移向量\n",
        "        b1_shape = (classes,)\n",
        "        b1_coeffs = weights_pred[:, start_index : start_index + np.prod(b1_shape)]\n",
        "        b1 = tf.reshape(b1_coeffs, b1_shape)\n",
        "        start_index += np.prod(b1_shape)\n",
        "\n",
        "        # 把这些权重赋给outer_model\n",
        "        outer_model.layers[0].kernel = w0\n",
        "        outer_model.layers[0].bias = b0\n",
        "        outer_model.layers[1].kernel = w1\n",
        "        outer_model.layers[1].bias = b1\n",
        "\n",
        "        # 用outer_model进行推断\n",
        "        preds = outer_model(x)\n",
        "        loss = loss_fn(y, preds)\n",
        "\n",
        "    # 只更新inner_model的权重\n",
        "    grads = tape.gradient(loss, inner_model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, inner_model.trainable_weights))\n",
        "    return loss\n",
        "\n",
        "\n",
        "losses = []  # 用于记录过程中产生的损失函数值\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "    loss = train_step(x, y)\n",
        "\n",
        "    # 输出日志\n",
        "    losses.append(float(loss))\n",
        "    if step % 100 == 0:\n",
        "        print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))\n",
        "\n",
        "    # 1000步之后停止\n",
        "    # 把模型训练至收敛就当成留给你的练习题了\n",
        "    if step >= 1000:\n",
        "        break"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0 Loss: 2.366020917892456\n",
            "Step: 100 Loss: 2.3394237100561655\n",
            "Step: 200 Loss: 2.1981412654937205\n",
            "Step: 300 Loss: 2.0647767751208472\n",
            "Step: 400 Loss: 1.937349613778005\n",
            "Step: 500 Loss: 1.884672182062808\n",
            "Step: 600 Loss: 1.822935720475254\n",
            "Step: 700 Loss: 1.749699467633804\n",
            "Step: 800 Loss: 1.703513056691031\n",
            "Step: 900 Loss: 1.634726283406388\n",
            "Step: 1000 Loss: 1.5674246165387933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixeE7egUNnEz"
      },
      "source": [
        "Keras是一个强有力的生产力工具，让你能轻松实现任何的科研想法。试想一下，你可能可以在一天之内尝试25个不同的想法（每20分钟一个）！\n",
        "\n",
        "Keras的宗旨之一就是从以最快的速度把想法变成结果，我们相信这是做出伟大科研成果的关键。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEn0MhKjNnEz"
      },
      "source": [
        "希望你喜欢这篇教程。记得告诉我们你用Keras做过什么有趣的事情。"
      ]
    }
  ]
}